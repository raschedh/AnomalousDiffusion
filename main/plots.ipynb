{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haidiri/Desktop/andi/andi_datasets/andi_datasets/utils_videos.py:22: UserWarning: From your imports it seems that you will need Deeptrack. Install if needed using pip install deeptrack.\n",
      "  warnings.warn('From your imports it seems that you will need Deeptrack. Install if needed using pip install deeptrack.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from andi_datasets.utils_challenge import *\n",
    "from andi_datasets.datasets_challenge import _get_dic_andi2, challenge_phenom_dataset\n",
    "from utils.padding import FEATURE_PADDING_VALUE, LABEL_PADDING_VALUE\n",
    "from utils.features import getFeatures\n",
    "from utils.plotting import *\n",
    "from utils.postprocessing import *\n",
    "from models.models import ClassificationModel, RegressionModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_svg import FigureCanvasSVG\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load predictions and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"<test dir>\"\n",
    "\n",
    "for file in os.listdir(ROOT_DIR):\n",
    "    path = os.path.join(ROOT_DIR, file)\n",
    "\n",
    "    if file == \"gt_a.npy\":\n",
    "        gt_a = np.load(path)\n",
    "    \n",
    "    if file == \"pred_a.npy\":\n",
    "        pred_a = np.load(path)\n",
    "    \n",
    "    if file == \"gt_k.npy\":\n",
    "        gt_k = np.load(path)\n",
    "    \n",
    "    if file == \"pred_k.npy\":\n",
    "        pred_k = np.load(path)\n",
    "    \n",
    "    if file == \"gt_state.npy\":\n",
    "        gt_state = np.load(path)\n",
    "    \n",
    "    if file == \"pred_state.npy\":\n",
    "        pred_state = np.load(path)\n",
    "\n",
    "print(gt_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error For K for Various CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps_k = {str(i): [] for i in range(6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_k.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 1000 tracks for all changepoint numbers\n",
    "    # if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "    #     break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    # if len(g) == 200: \n",
    "    if len(g)==200 and no_of_cps in range(6) and tracks_per_cp[str(no_of_cps)] < 10000:\n",
    "        total_tracks += 1\n",
    "        p = median_filter_1d(smooth_series(p, lower_limit=0, upper_limit=6))\n",
    "        mae = np.mean(np.abs(p - g))\n",
    "\n",
    "        d_cps_k[str(no_of_cps)].append(mae)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "plot_mae_cps(d_cps_k, label_type=\"k\")\n",
    "# mean_mae = {k: np.mean(v) if v else 0 for k, v in d_cps.items()}\n",
    "# print(mean_mae)\n",
    "# print(\"\\nFinal counts:\")\n",
    "# for k, v in tracks_per_cp.items():\n",
    "#     print(f\"Changepoints {k}: {v} tracks\")\n",
    "\n",
    "# print(\"\\nFinal d_cps lengths:\")\n",
    "# for k, v in d_cps.items():\n",
    "#     print(f\"Changepoints {k}: {len(v)} values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error Alpha for Various CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps_alpha = {str(i): [] for i in range(6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_a.copy()\n",
    "ground_truth = gt_a.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 1000 tracks for all changepoint numbers\n",
    "    # if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "    #     break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    if len(g) == 200 and no_of_cps in range(6) and tracks_per_cp[str(no_of_cps)] < 10000:\n",
    "        total_tracks += 1\n",
    "        p = median_filter_1d(smooth_series(p, lower_limit=0, upper_limit=1.999))\n",
    "        mae = np.mean(np.abs(p - g))\n",
    "\n",
    "        d_cps_alpha[str(no_of_cps)].append(mae)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "plot_mae_cps(d_cps_alpha)\n",
    "\n",
    "# mean_mae = {k: np.mean(v) if v else 0 for k, v in d_cps.items()}\n",
    "# print(mean_mae)\n",
    "# print(\"\\nFinal counts:\")\n",
    "# for k, v in tracks_per_cp.items():\n",
    "#     print(f\"Changepoints {k}: {v} tracks\")\n",
    "\n",
    "# print(\"\\nFinal d_cps lengths:\")\n",
    "# for k, v in d_cps.items():\n",
    "#     print(f\"Changepoints {k}: {len(v)} values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mae_cps_combined(d_cps_k, d_cps_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE for all combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps = {str(i): [] for i in range(1, 6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_a.copy()\n",
    "ground_truth = gt_a.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(1, 6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 1000 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    if len(g) == 200 and no_of_cps in range(1, 6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "        total_tracks += 1\n",
    "        \n",
    "        cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=1.999, threshold=0.05)\n",
    "        cp_gt = get_cps_gt(g)\n",
    "\n",
    "        cp_pred = cp_pred[1:-1]\n",
    "        cp_gt = cp_gt[1:-1]\n",
    "        \n",
    "        if cp_gt == cp_pred:\n",
    "            rmse = 0\n",
    "        else:\n",
    "            rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "        d_cps[str(no_of_cps)].append(rmse)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "results = {}\n",
    "for key, items in d_cps.items():\n",
    "    valid_items = [x for x in items if not (np.isnan(x) or np.isinf(x))]   \n",
    "    if valid_items:\n",
    "        avg_rmse = sum(valid_items) / len(valid_items)\n",
    "    else:\n",
    "        avg_rmse = float('nan')  # or handle empty case differently\n",
    "\n",
    "    results[key] = {\n",
    "        'average_rmse': avg_rmse,\n",
    "    }\n",
    "\n",
    "print(results)\n",
    "\n",
    "# plot_rmse_cps(d_cps, label_type=\"alpha_without_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps = {str(i): [] for i in range(1, 6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_k.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(1, 6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 1000 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    if len(g) == 200 and no_of_cps in range(1, 6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "        total_tracks += 1\n",
    "\n",
    "        cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=6, threshold=0.05)\n",
    "        cp_gt = get_cps_gt(g)\n",
    "\n",
    "        cp_pred = cp_pred[1:-1]\n",
    "        cp_gt = cp_gt[1:-1]\n",
    "\n",
    "        if cp_gt == cp_pred:\n",
    "            rmse = 0\n",
    "        else:\n",
    "            rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "        d_cps[str(no_of_cps)].append(rmse)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "plot_rmse_cps(d_cps, label_type=\"k_without_0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps = {str(i): [] for i in range(1, 6)}  # For storing Jaccard values\n",
    "tracks_per_cp = {str(i): 0 for i in range(1, 6)}  # For counting tracks per changepoint\n",
    "total_tracks = 0\n",
    "\n",
    "predictions = pred_state.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "\n",
    "for gt, pd in zip(ground_truth, predictions):\n",
    "    # Check if we have 100 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "        \n",
    "    gt = gt[gt!=LABEL_PADDING_VALUE]\n",
    "    pd = pd[:len(gt)]\n",
    "    \n",
    "    # Only process sequences of length 200\n",
    "    if len(gt) != 200:\n",
    "        continue\n",
    "        \n",
    "    pd = replace_short_sequences(pd, min_length=3)\n",
    "    \n",
    "    cp_pred = state_cps_function(pd)[1:-1]\n",
    "    cp_gt = get_cps_gt(gt)[1:-1]\n",
    "    no_of_cps = len(cp_gt)\n",
    "    \n",
    "    # Skip if we already have enough tracks for this number of changepoints\n",
    "    if no_of_cps in range(1, 6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "\n",
    "    # if no_of_cps >= 6 or no_of_cpstracks_per_cp[str(no_of_cps)] >= 100:\n",
    "    #     continue\n",
    "        \n",
    "        total_tracks += 1\n",
    "        # if no_of_cps == 5:\n",
    "        #     print(cp_pred, cp_gt)\n",
    "        \n",
    "        if cp_gt == cp_pred:\n",
    "            rmse = 0\n",
    "        else:\n",
    "            rmse, _ = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "        d_cps[str(no_of_cps)].append(rmse)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "plot_rmse_cps(d_cps, label_type=\"state_without_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for alpha + K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results and track counts\n",
    "d_cps = {str(i): [] for i in range(1, 6)}  # For storing Jaccard values\n",
    "tracks_per_cp = {str(i): 0 for i in range(1, 6)}  # For counting tracks per changepoint\n",
    "total_tracks = 0\n",
    "\n",
    "# Create copies of all predictions and ground truth arrays\n",
    "pred_a_data = pred_a.copy()\n",
    "gt_a_data = gt_a.copy()\n",
    "pred_k_data = pred_k.copy()\n",
    "gt_k_data = gt_k.copy()\n",
    "pred_state_data = pred_state.copy()\n",
    "gt_state_data = gt_state.copy()\n",
    "\n",
    "for i in range(len(gt_a_data)):\n",
    "    # Check if we have 100 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "    \n",
    "    # Get valid indices for all sequences\n",
    "    idx_a = padding_starts_index(gt_a_data[i])\n",
    "    \n",
    "    # Get predictions and ground truth for each model\n",
    "    p_alpha = pred_a_data[i][:idx_a]\n",
    "    g_alpha = gt_a_data[i][:idx_a]\n",
    "    \n",
    "    p_k = pred_k_data[i][:idx_a]\n",
    "    g_k = gt_k_data[i][:idx_a]\n",
    "    \n",
    "    p_state = pred_state_data[i][:idx_a]\n",
    "    g_state = gt_state_data[i][:idx_a]\n",
    "    \n",
    "    # Only process if sequence length is 200\n",
    "    if len(g_k) != 200:\n",
    "        continue\n",
    "    \n",
    "    # Count changepoints in ground truth (using any of the ground truth series)\n",
    "    no_of_cps = count_changepoints(g_k)\n",
    "    no_of_cps_alpha = count_changepoints(g_alpha)\n",
    "\n",
    "    if no_of_cps_alpha != no_of_cps:\n",
    "        continue\n",
    "    \n",
    "    # Skip if we already have enough tracks for this number of changepoints\n",
    "    if no_of_cps in range(1, 6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "    \n",
    "        total_tracks += 1\n",
    "        \n",
    "        # Get combined changepoints using the combined_cps function\n",
    "        merged_cps, _, _, _, _, _ = combined_cps_k_focused(p_alpha, p_k, p_state, include_state=True)\n",
    "        # merged_cps, _, _, _, _, _ = combined_cps_k_focused(p_alpha, p_k, p_state)\n",
    "        \n",
    "        merged_cps = [0] + merged_cps\n",
    "        merged_cps = merged_cps[1:-1]\n",
    "        \n",
    "        # Get ground truth changepoints (using any ground truth series as they should be the same)\n",
    "        cp_gt = get_cps_gt(g_k)\n",
    "        cp_gt = cp_gt[1:-1]  # Remove first and last points\n",
    "        # Compare predicted and ground truth changepoints\n",
    "        if cp_gt == merged_cps:\n",
    "            rmse = 0\n",
    "        else:\n",
    "            rmse, _ = single_changepoint_error(cp_gt, merged_cps)\n",
    "        \n",
    "        # Store results\n",
    "        d_cps[str(no_of_cps)].append(rmse)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "plot_rmse_cps(d_cps, label_type=\"alpha_k_state_focused_without_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for alpha + K + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_alpha_k_focused = np.load(\"rmse_values_alpha_k_focused_without_0_for_graph.npy\")\n",
    "rmse_alpha_k_state_k_focused = np.load(\"rmse_values_alpha_k_state_focused_without_0_for_graph.npy\")\n",
    "rmse_alpha = np.load(\"rmse_values_alpha_without_0_for_graph.npy\")\n",
    "rmse_k = np.load(\"rmse_values_k_without_0_for_graph.npy\")\n",
    "rmse_state = np.load(\"rmse_values_state_without_0_for_graph.npy\")\n",
    "\n",
    "n_cps = list(range(1,6))\n",
    "\n",
    "fig = Figure(figsize=(8, 6), dpi=300)\n",
    "canvas = FigureCanvasSVG(fig)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "lines = [\n",
    "    (rmse_k, K_COLOR, \"K\"),\n",
    "    (rmse_alpha, ALPHA_COLOR, \"α\"),\n",
    "    (rmse_state, STATE_COLOR, \"s\"),\n",
    "    (rmse_alpha_k_state_k_focused, AKS_COLOR, \"α+K+s\", '--', 'white'),  # Added linestyle and markerfacecolor\n",
    "    (rmse_alpha_k_focused, ALPHA_K_FOCUSED_COLOR, \"α+K\"),\n",
    "]\n",
    "\n",
    "for item in lines:\n",
    "    rmse_values, color, label, *style_args = item\n",
    "    linestyle = style_args[0] if len(style_args) > 0 else '-'\n",
    "    markerfacecolor = style_args[1] if len(style_args) > 1 else color\n",
    "    \n",
    "    ax.plot(n_cps, rmse_values, \n",
    "            color=color,\n",
    "            linewidth=2,\n",
    "            alpha=1,\n",
    "            marker='o',\n",
    "            markersize=10,\n",
    "            markerfacecolor=markerfacecolor,\n",
    "            markeredgecolor=color,\n",
    "            markeredgewidth=2,\n",
    "            linestyle=linestyle,\n",
    "            label=label)\n",
    "\n",
    "all_values = np.concatenate([\n",
    "    rmse_alpha_k_focused,\n",
    "    rmse_alpha_k_state_k_focused,\n",
    "    rmse_alpha,\n",
    "    rmse_k,\n",
    "    rmse_state\n",
    "])\n",
    "valid_values = all_values[~(np.isnan(all_values) | np.isinf(all_values))]\n",
    "\n",
    "if len(valid_values) > 0:\n",
    "    ymin = np.min(valid_values)\n",
    "    ymax = np.max(valid_values)\n",
    "    margin = (ymax - ymin) * 0.1\n",
    "    ax.set_ylim(ymin - margin, ymax + 1.5* margin)\n",
    "\n",
    "ax.set_xticks(n_cps)\n",
    "ax.tick_params(which='both', direction='out', length=6, width=1,\n",
    "                colors='black', pad=2, labelsize=32)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_linewidth(1)\n",
    "    spine.set_color('black')\n",
    "\n",
    "ax.set_xlabel('$N_{\\mathrm{CP}}$', fontsize=32)\n",
    "ax.set_ylabel('RMSE', fontsize=32, rotation=90, va='center', labelpad=20)\n",
    "\n",
    "legend = ax.legend(fontsize=28, frameon=True,\n",
    "                    loc='upper right',\n",
    "                    bbox_to_anchor=(0.98, 0.98),\n",
    "                    edgecolor='none',\n",
    "                    facecolor='white',\n",
    "                    framealpha=0.8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.patch.set_facecolor('none')\n",
    "ax.set_facecolor('none')\n",
    "\n",
    "canvas.print_figure('all_methods_rmse_comparison_without_0.svg',\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0.1,\n",
    "                    format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changepoint for Various Positions Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bins for different positions\n",
    "bin_size = 20  # Assuming this is your bin size\n",
    "num_bins = 200 // bin_size\n",
    "d_positions = {str(i * bin_size): [] for i in range(num_bins)}\n",
    "tracks_per_bin = {str(i * bin_size): 0 for i in range(num_bins)}\n",
    "total_tracks = 0\n",
    "\n",
    "predictions = pred_a.copy()\n",
    "ground_truth = gt_a.copy()\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    if all(count >= 100 for count in tracks_per_bin.values()):\n",
    "        break\n",
    "    \n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    # Only process if sequence length is 200\n",
    "    if len(g) == 200:\n",
    "        \n",
    "        cp_gt = get_cps_gt(g)[1:-1]\n",
    "\n",
    "        if len(cp_gt) == 1:\n",
    "\n",
    "            cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=1.999, threshold=0.05)\n",
    "            cp_pred = cp_pred[1:-1]        \n",
    "\n",
    "            cp_position = cp_gt[0]\n",
    "            bin_start = str((cp_position // bin_size) * bin_size)\n",
    "            \n",
    "            # Only process if we need more tracks for this position bin\n",
    "            if bin_start in tracks_per_bin and tracks_per_bin[bin_start] < 100:\n",
    "                # Calculate Jaccard index\n",
    "                if cp_gt == cp_pred:\n",
    "                    jaccard_value = 1\n",
    "                else:\n",
    "                    rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "                \n",
    "                # Store the result\n",
    "                d_positions[bin_start].append(jaccard_value)\n",
    "                tracks_per_bin[bin_start] += 1\n",
    "                total_tracks += 1\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Total tracks: {total_tracks} | Tracks per bin:\", end=\" \")\n",
    "                for k, v in tracks_per_bin.items():\n",
    "                    print(f\"{k}:{v}\", end=\" \")\n",
    "                print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have it saved load from file\n",
    "# with open('changepoint_position_jaccard_value_alpha.pkl', 'wb') as f:\n",
    "#     pickle.dump(d_positions, f)\n",
    "\n",
    "plot_jaccard_position(d_positions, label_type=\"alpha\", bin_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_positions = {}  # Dictionary to store jaccard values by position bins\n",
    "# bin_size = 20  # Size of position bins\n",
    "# predictions = pred_a.copy()\n",
    "# ground_truth = gt_a.copy()\n",
    "# tracks = 0\n",
    "\n",
    "# for i in range(len(ground_truth)):\n",
    "#     idx = padding_starts_index(ground_truth[i])\n",
    "#     p = predictions[i][:idx]\n",
    "#     g = ground_truth[i][:idx]\n",
    "    \n",
    "#     no_of_cps = count_changepoints(g)\n",
    "\n",
    "#     # Only look at tracks with length 200 and exactly 1 changepoint\n",
    "#     if len(g) == 200 and no_of_cps == 1:\n",
    "#         tracks += 1\n",
    "        \n",
    "#         if tracks > 2000:\n",
    "#             break\n",
    "\n",
    "#         cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=1.999, threshold=0.05)\n",
    "#         cp_gt = get_cps_gt(g)\n",
    "\n",
    "#         # Calculate Jaccard index\n",
    "#         cp_pred = cp_pred[1:-1]\n",
    "#         cp_gt = cp_gt[1:-1]\n",
    "\n",
    "#         position_bin = (cp_gt[0] - 1) // bin_size * bin_size  # Subtract 1 to start from 0\n",
    "#         bin_label = str(position_bin)\n",
    "\n",
    "#         if cp_gt == cp_pred:\n",
    "#             jaccard_value = 1\n",
    "#         else:\n",
    "#             rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "\n",
    "#         # Store in position-based dictionary\n",
    "#         if bin_label not in d_positions:\n",
    "#             d_positions[bin_label] = [jaccard_value]\n",
    "#         else:\n",
    "#             d_positions[bin_label].append(jaccard_value)\n",
    "\n",
    "#         print(tracks, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks = 0\n",
    "# for i in range(len(ground_truth)):\n",
    "#     idx = padding_starts_index(ground_truth[i])\n",
    "#     p = predictions[i][:idx]\n",
    "#     g = ground_truth[i][:idx]\n",
    "    \n",
    "#     no_of_cps = count_changepoints(g)\n",
    "\n",
    "#     # Only look at tracks with length 200 and exactly 1 changepoint\n",
    "#     if len(g) == 200 and no_of_cps == 1:\n",
    "#         tracks += 1\n",
    "#         if tracks == 200:\n",
    "#             plt.scatter([i for i in range(len(p))], p)  \n",
    "#             plt.scatter([i for i in range(len(g))], g)\n",
    "#             break\n",
    "# plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average Jaccard value for each position bin\n",
    "results = {}\n",
    "for key, items in d_positions.items():\n",
    "    avg_jaccard = sum(items) / len(items)\n",
    "    num_tracks = len(items)\n",
    "    results[key] = {\n",
    "        'average_jaccard': avg_jaccard,\n",
    "        'num_tracks': num_tracks,\n",
    "        'all_values': items\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults by position bin:\")\n",
    "for key in sorted(results.keys(), key=int):\n",
    "    print(f\"\\nPosition bin {key}-{int(key)+bin_size}:\")\n",
    "    print(f\"Average Jaccard value: {results[key]['average_jaccard']:.4f}\")\n",
    "    print(f\"Number of tracks: {results[key]['num_tracks']}\")\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract x and y values for plotting\n",
    "x_values = [int(key) + bin_size/2 for key in sorted(results.keys(), key=int)]  # Use bin centers\n",
    "y_values = [results[key]['average_jaccard'] for key in sorted(results.keys(), key=int)]\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(x_values, y_values, color='blue', s=100)\n",
    "\n",
    "# Add value labels above each point\n",
    "for i, v in enumerate(y_values):\n",
    "    plt.text(x_values[i], v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.xlabel('Changepoint Position')\n",
    "plt.ylabel('Average Jaccard Value')\n",
    "plt.title('Average Jaccard Values by Changepoint Position For Alpha')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Set x-ticks to show bin edges\n",
    "bin_edges = range(0, 201, bin_size)\n",
    "plt.xticks(bin_edges)\n",
    "\n",
    "# Set y-axis limits to start from 0 and have some padding at the top\n",
    "plt.ylim(0, max(y_values) + 0.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"jaccard_vs_position_test_set_alpha.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changepoints for Various Positions K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bins for different positions\n",
    "bin_size = 20\n",
    "num_bins = 200 // bin_size\n",
    "d_positions = {str(i * bin_size): [] for i in range(num_bins)}\n",
    "tracks_per_bin = {str(i * bin_size): 0 for i in range(num_bins)}\n",
    "total_tracks = 0\n",
    "\n",
    "predictions = pred_k.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 100 tracks for all position bins\n",
    "    if all(count >= 100 for count in tracks_per_bin.values()):\n",
    "        break\n",
    "    \n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    # Only process if sequence length is 200\n",
    "    if len(g) == 200:\n",
    "        \n",
    "        cp_gt = get_cps_gt(g)[1:-1]\n",
    "\n",
    "        if len(cp_gt) == 1:\n",
    "            cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=6, threshold=0.05)  # Changed upper_limit for k\n",
    "            cp_pred = cp_pred[1:-1]        \n",
    "\n",
    "            cp_position = cp_gt[0]\n",
    "            bin_start = str((cp_position // bin_size) * bin_size)\n",
    "            \n",
    "            # Only process if we need more tracks for this position bin\n",
    "            if bin_start in tracks_per_bin and tracks_per_bin[bin_start] < 100:\n",
    "                # Calculate Jaccard index\n",
    "                if cp_gt == cp_pred:\n",
    "                    jaccard_value = 1\n",
    "                else:\n",
    "                    rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "                \n",
    "                # Store the result\n",
    "                d_positions[bin_start].append(jaccard_value)\n",
    "                tracks_per_bin[bin_start] += 1\n",
    "                total_tracks += 1\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Total tracks: {total_tracks} | Tracks per bin:\", end=\" \")\n",
    "                for k, v in tracks_per_bin.items():\n",
    "                    print(f\"{k}:{v}\", end=\" \")\n",
    "                print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if saved load from file\n",
    "# with open('changepoint_position_jaccard_value_k.pkl', 'wb') as f:\n",
    "#     pickle.dump(d_positions, f)\n",
    "\n",
    "plot_jaccard_position(d_positions, label_type=\"k\", bin_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changepoints for Various Positions State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bins for different positions\n",
    "bin_size = 20\n",
    "num_bins = 200 // bin_size\n",
    "d_positions = {str(i * bin_size): [] for i in range(num_bins)}\n",
    "tracks_per_bin = {str(i * bin_size): 0 for i in range(num_bins)}\n",
    "total_tracks = 0\n",
    "\n",
    "predictions = pred_state.copy()\n",
    "ground_truth = gt_state.copy()\n",
    "\n",
    "for gt, pd in zip(ground_truth, predictions):\n",
    "    # Check if we have 100 tracks for all position bins\n",
    "    if all(count >= 100 for count in tracks_per_bin.values()):\n",
    "        break\n",
    "        \n",
    "    gt = gt[gt!=LABEL_PADDING_VALUE]\n",
    "    pd = pd[:len(gt)]\n",
    "    \n",
    "    # Only process if sequence length is 200\n",
    "    if len(gt) == 200:\n",
    "        pd = replace_short_sequences(pd, min_length=3)\n",
    "        \n",
    "        cp_gt = get_cps_gt(gt)[1:-1]\n",
    "\n",
    "        if len(cp_gt) == 1:  # Only process sequences with exactly one changepoint\n",
    "            cp_pred = state_cps_function(pd)[1:-1]\n",
    "\n",
    "            cp_position = cp_gt[0]\n",
    "            bin_start = str((cp_position // bin_size) * bin_size)\n",
    "            \n",
    "            # Only process if we need more tracks for this position bin\n",
    "            if bin_start in tracks_per_bin and tracks_per_bin[bin_start] < 100:\n",
    "                # Calculate Jaccard index\n",
    "                if cp_gt == cp_pred:\n",
    "                    jaccard_value = 1\n",
    "                else:\n",
    "                    rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "                \n",
    "                # Store the result\n",
    "                d_positions[bin_start].append(jaccard_value)\n",
    "                tracks_per_bin[bin_start] += 1\n",
    "                total_tracks += 1\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Total tracks: {total_tracks} | Tracks per bin:\", end=\" \")\n",
    "                for k, v in tracks_per_bin.items():\n",
    "                    print(f\"{k}:{v}\", end=\" \")\n",
    "                print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if saved load from file\n",
    "# with open('changepoint_position_jaccard_value_state.pkl', 'wb') as f:\n",
    "#     pickle.dump(d_positions, f)\n",
    "\n",
    "plot_jaccard_position(d_positions, label_type=\"state\", bin_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changepoints for All variables Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (when you need it later)\n",
    "with open('plots/changepoint_position_jaccard_value_k.pkl', 'rb') as f:\n",
    "    d_positions_k = pickle.load(f)\n",
    "\n",
    "# Load (when you need it later)\n",
    "with open('plots/changepoint_position_jaccard_value_state.pkl', 'rb') as f:\n",
    "    d_positions_state = pickle.load(f)\n",
    "\n",
    "# Load (when you need it later)\n",
    "with open('plots/changepoint_position_jaccard_value_alpha.pkl', 'rb') as f:\n",
    "    d_positions_alpha = pickle.load(f)\n",
    "\n",
    "fig, ax = plot_multiple_position_jaccard(d_positions_alpha, d_positions_k, d_positions_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha for Various CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps = {str(i): [] for i in range(6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_a.copy()\n",
    "ground_truth = gt_a.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 1000 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    if len(g) == 200 and no_of_cps in range(6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "        total_tracks += 1\n",
    "        \n",
    "        cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=1.999, threshold=0.05)\n",
    "        cp_gt = get_cps_gt(g)\n",
    "\n",
    "        cp_pred = cp_pred[1:-1]\n",
    "        cp_gt = cp_gt[1:-1]\n",
    "\n",
    "        if cp_gt == cp_pred:\n",
    "            jaccard_value = 1\n",
    "        else:\n",
    "            rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "        d_cps[str(no_of_cps)].append(jaccard_value)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "# print(\"\\nFinal counts:\")\n",
    "# for k, v in tracks_per_cp.items():\n",
    "#     print(f\"Changepoints {k}: {v} tracks\")\n",
    "\n",
    "# print(\"\\nFinal d_cps lengths:\")\n",
    "# for k, v in d_cps.items():\n",
    "#     print(f\"Changepoints {k}: {len(v)} values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jaccard_cps(d_cps, label_type=\"alpha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K for Various CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cps = {str(i): [] for i in range(6)}  # Initialize with all possible changepoint numbers\n",
    "\n",
    "predictions = pred_k.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # Track count for each number of changepoints\n",
    "total_tracks = 0\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    # Check if we have 100 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "\n",
    "    idx = padding_starts_index(ground_truth[i])\n",
    "    p = predictions[i][:idx]\n",
    "    g = ground_truth[i][:idx]\n",
    "    \n",
    "    no_of_cps = count_changepoints(g)\n",
    "    no_of_cps_alpha = count_changepoints(gt_a[i][:idx])\n",
    "\n",
    "    if no_of_cps_alpha != no_of_cps:\n",
    "        continue\n",
    "    \n",
    "    # Only process if we need more tracks for this number of changepoints\n",
    "    if len(g) == 200 and no_of_cps in range(6) and tracks_per_cp[str(no_of_cps)] < 100:\n",
    "        total_tracks += 1\n",
    "        \n",
    "        cp_pred, _ = get_cps_ruptures(p, lower_limit=0, upper_limit=6, threshold=0.05)  # Changed upper limit to 6 for K\n",
    "        cp_gt = get_cps_gt(g)\n",
    "\n",
    "        cp_pred = cp_pred[1:-1]\n",
    "        cp_gt = cp_gt[1:-1]\n",
    "\n",
    "        if cp_gt == cp_pred:\n",
    "            jaccard_value = 1\n",
    "        else:\n",
    "            rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "        d_cps[str(no_of_cps)].append(jaccard_value)\n",
    "        tracks_per_cp[str(no_of_cps)] += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "        for k, v in tracks_per_cp.items():\n",
    "            print(f\"{k}:{v}\", end=\" \")\n",
    "        print(\"\", end=\"\\r\")\n",
    "\n",
    "# print(\"\\nFinal counts:\")\n",
    "# for k, v in tracks_per_cp.items():\n",
    "#     print(f\"Changepoints {k}: {v} tracks\")\n",
    "\n",
    "# print(\"\\nFinal d_cps lengths:\")\n",
    "# for k, v in d_cps.items():\n",
    "#     print(f\"Changepoints {k}: {len(v)} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jaccard_cps(d_cps, label_type=\"k_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Various CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results and track counts\n",
    "d_cps = {str(i): [] for i in range(6)}  # For storing Jaccard values\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # For counting tracks per changepoint\n",
    "total_tracks = 0\n",
    "\n",
    "predictions = pred_state.copy()\n",
    "ground_truth = gt_k.copy()\n",
    "\n",
    "for gt, pd in zip(ground_truth, predictions):\n",
    "    # Check if we have 100 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "        \n",
    "    gt = gt[gt!=LABEL_PADDING_VALUE]\n",
    "    pd = pd[:len(gt)]\n",
    "    \n",
    "    # Only process sequences of length 200\n",
    "    if len(gt) != 200:\n",
    "        continue\n",
    "        \n",
    "    pd = replace_short_sequences(pd, min_length=3)\n",
    "    \n",
    "    cp_pred = state_cps_function(pd)[1:-1]\n",
    "    cp_gt = get_cps_gt(gt)[1:-1]\n",
    "\n",
    "    # Count the number of changepoints in ground truth\n",
    "    no_of_cps = len(cp_gt)\n",
    "    \n",
    "    # Skip if we already have enough tracks for this number of changepoints\n",
    "    if no_of_cps >= 6 or tracks_per_cp[str(no_of_cps)] >= 100:\n",
    "        continue\n",
    "        \n",
    "    total_tracks += 1\n",
    "    \n",
    "    if cp_gt == cp_pred:\n",
    "        jaccard_value = 1\n",
    "    else:\n",
    "        rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "        \n",
    "    d_cps[str(no_of_cps)].append(jaccard_value)\n",
    "    tracks_per_cp[str(no_of_cps)] += 1\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "    for k, v in tracks_per_cp.items():\n",
    "        print(f\"{k}:{v}\", end=\" \")\n",
    "    print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jaccard_cps(d_cps, label_type=\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all variable for changepoint detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results and track counts\n",
    "d_cps = {str(i): [] for i in range(6)}  # For storing Jaccard values\n",
    "tracks_per_cp = {str(i): 0 for i in range(6)}  # For counting tracks per changepoint\n",
    "total_tracks = 0\n",
    "\n",
    "# Create copies of all predictions and ground truth arrays\n",
    "pred_a_data = pred_a.copy()\n",
    "gt_a_data = gt_a.copy()\n",
    "pred_k_data = pred_k.copy()\n",
    "gt_k_data = gt_k.copy()\n",
    "pred_state_data = pred_state.copy()\n",
    "gt_state_data = gt_state.copy()\n",
    "\n",
    "for i in range(len(gt_a_data)):\n",
    "    # Check if we have 100 tracks for all changepoint numbers\n",
    "    if all(count >= 100 for count in tracks_per_cp.values()):\n",
    "        break\n",
    "    \n",
    "    # Get valid indices for all sequences\n",
    "    idx_a = padding_starts_index(gt_a_data[i])\n",
    "    \n",
    "    # Get predictions and ground truth for each model\n",
    "    p_alpha = pred_a_data[i][:idx_a]\n",
    "    g_alpha = gt_a_data[i][:idx_a]\n",
    "    \n",
    "    p_k = pred_k_data[i][:idx_a]\n",
    "    g_k = gt_k_data[i][:idx_a]\n",
    "    \n",
    "    p_state = pred_state_data[i][:idx_a]\n",
    "    g_state = gt_state_data[i][:idx_a]\n",
    "    \n",
    "    # Only process if sequence length is 200\n",
    "    if len(g_k) != 200:\n",
    "        continue\n",
    "    \n",
    "    # Count changepoints in ground truth (using any of the ground truth series)\n",
    "    no_of_cps = count_changepoints(g_k)\n",
    "    no_of_cps_alpha = count_changepoints(g_alpha)\n",
    "\n",
    "    if no_of_cps_alpha != no_of_cps:\n",
    "        continue\n",
    "    \n",
    "    # Skip if we already have enough tracks for this number of changepoints\n",
    "    if no_of_cps >= 6 or tracks_per_cp[str(no_of_cps)] >= 100:\n",
    "        continue\n",
    "    \n",
    "    total_tracks += 1\n",
    "    \n",
    "    # Get combined changepoints using the combined_cps function\n",
    "    # combined_cps_k_focused_with_state\n",
    "    # merged_cps, _, _, _, _, _ = combined_cps_with_state(p_alpha, p_k, p_state)\n",
    "    merged_cps, _, _, _, _, _ = combined_cps_k_focused(p_alpha, p_k, p_state, include_state=True)\n",
    "\n",
    "    # merged_cps, _, _, _, _, _ = combined_cps_k_focused(p_alpha, p_k, window_size=5)\n",
    "    # merged_cps, _, _, _, _ = combined_cps(p_alpha, p_k)\n",
    "    \n",
    "    merged_cps = [0] + merged_cps\n",
    "    merged_cps = merged_cps[1:-1]\n",
    "    \n",
    "    # Get ground truth changepoints (using any ground truth series as they should be the same)\n",
    "    cp_gt = get_cps_gt(g_k)\n",
    "    cp_gt = cp_gt[1:-1]  # Remove first and last points\n",
    "    \n",
    "    # Compare predicted and ground truth changepoints\n",
    "    if cp_gt == merged_cps:\n",
    "        jaccard_value = 1\n",
    "        rmse = 0\n",
    "    else:\n",
    "        rmse, jaccard_value = single_changepoint_error(cp_gt, merged_cps)\n",
    "    \n",
    "    # Store results\n",
    "    d_cps[str(no_of_cps)].append(jaccard_value)\n",
    "    tracks_per_cp[str(no_of_cps)] += 1\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Total tracks: {total_tracks} | Tracks per CP:\", end=\" \")\n",
    "    for k, v in tracks_per_cp.items():\n",
    "        print(f\"{k}:{v}\", end=\" \")\n",
    "    print(\"\", end=\"\\r\")\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nFinal counts:\")\n",
    "for k, v in tracks_per_cp.items():\n",
    "    print(f\"Changepoints {k}: {v} tracks\")\n",
    "\n",
    "print(\"\\nFinal d_cps lengths:\")\n",
    "for k, v in d_cps.items():\n",
    "    print(f\"Changepoints {k}: {len(v)} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_jaccard_cps(d_cps, label_type=\"alpha_k_state_k_focused\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have all can plot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_alpha_k_focused = np.load(\"plots/jaccard_values_alpha_and_k_focused_for_graph.npy\")\n",
    "# j_alpha_k = np.load(\"jaccard_values_alpha_and_k_for_graph.npy\")\n",
    "j_alpha_k_state_k_focused = np.load(\"plots/jaccard_values_alpha_k_state_k_focused_for_graph.npy\")\n",
    "# j_alpha_k_state = np.load(\"jaccard_values_alpha_k_state_for_graph.npy\")\n",
    "j_alpha = np.load(\"plots/jaccard_values_alpha_for_graph.npy\")\n",
    "j_k = np.load(\"plots/jaccard_values_k_for_graph.npy\")\n",
    "j_state = np.load(\"plots/jaccard_values_state_new_for_graph.npy\")\n",
    "\n",
    "# Create x-axis values\n",
    "n_cps = [i for i in range(6)]\n",
    "\n",
    "# Create figure with specific DPI\n",
    "fig = Figure(figsize=(8, 6), dpi=300)\n",
    "canvas = FigureCanvasSVG(fig)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "lines = [\n",
    "    (j_k, K_COLOR, \"K\"),\n",
    "    (j_alpha, ALPHA_COLOR, \"α\"),\n",
    "    (j_state, STATE_COLOR, \"s\"),\n",
    "    (j_alpha_k_state_k_focused, \"black\", \"α+K+s\", '--', 'white'),\n",
    "    (j_alpha_k_focused, \"black\", \"α+K\")\n",
    "    ]\n",
    "\n",
    "for item in lines:\n",
    "    jaccard_values, color, label, *style_args = item\n",
    "    linestyle = style_args[0] if len(style_args) > 0 else '-'\n",
    "    markerfacecolor = style_args[1] if len(style_args) > 1 else color\n",
    "\n",
    "    ax.plot(n_cps, jaccard_values,\n",
    "    color=color,\n",
    "    linewidth=2,\n",
    "    alpha=1,\n",
    "    marker='o',\n",
    "    markersize=10,\n",
    "    markerfacecolor=markerfacecolor,\n",
    "    markeredgecolor=color,\n",
    "    markeredgewidth=2,\n",
    "    linestyle=linestyle,\n",
    "    label=label)\n",
    "\n",
    "# Set y-axis limits with margin\n",
    "all_values = np.concatenate([j_alpha_k_focused, \n",
    "# j_alpha_k, \n",
    "j_alpha_k_state_k_focused, \n",
    "# j_alpha_k_state, \n",
    "j_alpha, \n",
    "j_k, \n",
    "j_state])\n",
    "ymin = np.min(all_values)\n",
    "ymax = np.max(all_values)\n",
    "margin = (ymax - ymin) * 0.1\n",
    "\n",
    "# ax.set_ylim(ymin - margin, ymax + margin)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_yticks([0, 0.5, 1])\n",
    "ax.set_yticklabels(['0', '0.5', '1'])\n",
    "# Set x-axis to show only integer values\n",
    "ax.set_xticks(n_cps)\n",
    "\n",
    "\n",
    "# Configure axis and style\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Configure tick parameters\n",
    "ax.tick_params(which='both', direction='out', length=6, width=1,\n",
    "colors='black', pad=2)\n",
    "\n",
    "# Set tick label sizes\n",
    "ax.tick_params(axis='both', labelsize=32)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_linewidth(1)\n",
    "    spine.set_color('black')\n",
    "\n",
    "ax.set_xlabel('$N_{\\mathrm{CP}}$', fontsize=32)\n",
    "ax.set_ylabel(r'$\\overline{J}$', fontsize=32, rotation=0, va='center', labelpad=20)\n",
    "\n",
    "legend = ax.legend(fontsize=28, frameon=True,\n",
    "loc='upper right',\n",
    "bbox_to_anchor=(0.98, 0.98),\n",
    "edgecolor='none',\n",
    "facecolor='white',\n",
    "framealpha=0.8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.patch.set_facecolor('none')\n",
    "ax.set_facecolor('none')\n",
    "\n",
    "canvas.print_figure('all_methods_jaccard_comparison_new.svg',\n",
    "bbox_inches='tight',\n",
    "pad_inches=0.1,\n",
    "format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPs for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models in the correct order\n",
    "# Constants\n",
    "d_indices = {model: [[] for _ in range(NO_OF_CPS_MAX + 1)] for model in MODELS}\n",
    "\n",
    "# Get masks for valid tracks\n",
    "sequence_lengths = (gt_k != LABEL_PADDING_VALUE).sum(axis=1)\n",
    "length_mask = sequence_lengths == 200\n",
    "\n",
    "changes = gt_k[:, 1:] != gt_k[:, :-1]\n",
    "changepoints = changes.sum(axis=1)\n",
    "changepoint_mask = (changepoints >= 0) & (changepoints <= NO_OF_CPS_MAX)\n",
    "\n",
    "final_mask = changepoint_mask & length_mask\n",
    "valid_indices = np.where(final_mask)[0]\n",
    "\n",
    "# First pass: categorize all valid tracks\n",
    "for idx in valid_indices:\n",
    "    cp_gt = np.where(gt_k[idx, 1:] != gt_k[idx, :-1])[0] + 1\n",
    "    no_of_cps = len(cp_gt)\n",
    "    states = gt_state[idx]\n",
    "    \n",
    "    # Determine model\n",
    "    if 1 in states:\n",
    "        model = \"conf.\"\n",
    "    elif 0 in states:\n",
    "        model = \"imm.\"\n",
    "    elif 3 in states:\n",
    "        model = \"dir.\"\n",
    "    else:\n",
    "        model = \"free\"\n",
    "        \n",
    "    d_indices[model][no_of_cps].append(idx)\n",
    "\n",
    "# Initialize result dictionary\n",
    "d_cps = {model: [[] for _ in range(NO_OF_CPS_MAX + 1)] for model in MODELS}\n",
    "\n",
    "# Process tracks with balanced sampling\n",
    "progress_bar = tqdm(desc=\"Processing tracks\", total=len(MODELS) * (NO_OF_CPS_MAX + 1) * TRACKS_PER_CATEGORY)\n",
    "\n",
    "for model in MODELS:\n",
    "    for no_of_cps in range(NO_OF_CPS_MAX + 1):\n",
    "        available_indices = d_indices[model][no_of_cps]\n",
    "        \n",
    "        # If we don't have enough tracks, sample with replacement\n",
    "        if len(available_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        indices_to_process = np.random.choice(\n",
    "            available_indices,\n",
    "            size=TRACKS_PER_CATEGORY,\n",
    "            replace=len(available_indices) < TRACKS_PER_CATEGORY\n",
    "        )\n",
    "        \n",
    "        for idx in indices_to_process:\n",
    "\n",
    "            cp_gt = np.where(gt_k[idx, 1:] != gt_k[idx, :-1])[0] + 1\n",
    "            pk = pred_k[idx]\n",
    "            pa = pred_a[idx]\n",
    "            ps = pred_state[idx]\n",
    "            \n",
    "            # Calculate changepoints and Jaccard\n",
    "            # cp_pred, _ = get_cps_ruptures(pk, lower_limit=0, upper_limit=6, threshold=0.05)\n",
    "            cp_pred, _, _, _, _, _ = combined_cps_k_focused(pa, pk, ps)\n",
    "            cp_pred = [0] + cp_pred\n",
    "            cp_pred = cp_pred[1:-1]\n",
    "            \n",
    "            if list(cp_gt) == list(cp_pred):\n",
    "                jaccard_value = 1\n",
    "            else:\n",
    "                rmse, jaccard_value = single_changepoint_error(cp_gt, cp_pred)\n",
    "            \n",
    "            d_cps[model][no_of_cps].append(jaccard_value)\n",
    "            progress_bar.update()\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Print statistics about the sampling\n",
    "print(\"\\nSampling Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for model in MODELS:\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for no_of_cps in range(NO_OF_CPS_MAX + 1):\n",
    "        original_count = len(d_indices[model][no_of_cps])\n",
    "        sampled_count = len(d_cps[model][no_of_cps])\n",
    "        print(f\"  CPs={no_of_cps}: Original={original_count}, Sampled={sampled_count}\")\n",
    "        if original_count < TRACKS_PER_CATEGORY and original_count > 0:\n",
    "            print(f\"    (Sampled with replacement to reach target)\")\n",
    "\n",
    "# Save\n",
    "with open('jaccard_for_models.json', 'w') as f:\n",
    "    json.dump(d_cps, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data if already saved \n",
    "# with open('jaccard_for_models.json', 'r') as f:\n",
    "#    d_cps = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for no_of_cps in range(NO_OF_CPS_MAX + 1):\n",
    "        sampled_count = len(d_cps[model][no_of_cps])\n",
    "        print(f\"  CPs={no_of_cps}: Original={original_count}, Sampled={sampled_count}\")\n",
    "        if original_count < TRACKS_PER_CATEGORY and original_count > 0:\n",
    "            print(f\"    (Sampled with replacement to reach target)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_jaccard(d_cps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "model = []  # Clear your data lists\n",
    "jaccard_values = []\n",
    "# Calculate average Jaccard value for each number of changepoints\n",
    "results = {}\n",
    "for key, items in d_cps.items():\n",
    "    avg_jaccard = sum(items) / len(items)\n",
    "    num_tracks = len(items)\n",
    "    results[key] = {\n",
    "        'average_jaccard': avg_jaccard,\n",
    "        'num_tracks': num_tracks,\n",
    "        'all_values': items\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults by number of changepoints:\")\n",
    "for key in sorted(results.keys()):\n",
    "    print(f\"\\nNumber of changepoints: {key}\")\n",
    "    print(f\"Average Jaccard value: {results[key]['average_jaccard']:.4f}\")\n",
    "    print(f\"Number of tracks: {results[key]['num_tracks']}\")\n",
    "\n",
    "# Optionally, create lists for plotting\n",
    "for key in sorted(results.keys()):\n",
    "    delta_alpha.append(int(key))\n",
    "    jaccard_values.append(results[key]['average_jaccard'])\n",
    "# You can now use delta_alpha and jaccard_values for plotting if needed\n",
    "# Example plot:\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(delta_alpha, jaccard_values, marker='o')\n",
    "plt.xlabel('Number of Changepoints')\n",
    "plt.ylabel('Average Jaccard Value')\n",
    "plt.title('Jaccard Values vs Number of Changepoints for K')\n",
    "plt.grid(True)\n",
    "plt.savefig(\"jaccard_vs_changepoints_test_set_for_k_2.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error in K and Alpha Per State/Model\n",
    "\n",
    "- Given the ground truth state, works out error in K and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_k = {'imm.': [], 'conf.': [], 'free': [], 'dir.': []}\n",
    "errors_a = {'imm.': [], 'conf.': [], 'free': [], 'dir.': []}\n",
    "\n",
    "# Create mask for valid entries\n",
    "valid_mask = np.all(gt_state != LABEL_PADDING_VALUE, axis=1)\n",
    "\n",
    "pk2 = np.clip(pred_k, 0, 6)\n",
    "pa2 = np.clip(pred_a, 0, 2)\n",
    "\n",
    "# Calculate absolute errors\n",
    "k_error = np.abs(gt_k - pk2).sum(axis=1)/200\n",
    "a_error = np.abs(gt_a - pa2).sum(axis=1)/200\n",
    "\n",
    "# Group errors by state\n",
    "for idx in range(len(gt_state)):\n",
    "    if valid_mask[idx]:\n",
    "        states = gt_state[idx]\n",
    "        k_err = k_error[idx]\n",
    "        a_err = a_error[idx]\n",
    "        \n",
    "        if 1 in states:\n",
    "            errors_k['conf.'].append(k_err)\n",
    "            errors_a['conf.'].append(a_err)\n",
    "        elif 0 in states:\n",
    "            errors_k['imm.'].append(k_err)\n",
    "            errors_a['imm.'].append(a_err)\n",
    "        elif 3 in states:\n",
    "            errors_k['dir.'].append(k_err)\n",
    "            errors_a['dir.'].append(a_err)\n",
    "        else:\n",
    "            errors_k['free'].append(k_err)\n",
    "            errors_a['free'].append(a_err)\n",
    "\n",
    "# Calculate MAE for each state\n",
    "mae_k = {state: np.mean(errs) for state, errs in errors_k.items()}\n",
    "mae_a = {state: np.mean(errs) for state, errs in errors_a.items()}\n",
    "\n",
    "plot_errors_by_model(mae_k, mae_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize error lists\n",
    "# errors_k = [[] for _ in range(NUM_CLASSES)]\n",
    "# errors_alpha = [[] for _ in range(NUM_CLASSES)]\n",
    "\n",
    "# # Clip predictions\n",
    "# # pred_k = np.clip(pred_k, 0, 6)\n",
    "# # pred_a = np.clip(pred_a, 0, 2)\n",
    "\n",
    "# pk = median_filter_1d(smooth_series(pred_k, lower_limit=0, upper_limit=6))\n",
    "# pa = median_filter_1d(smooth_series(pred_a, lower_limit=0, upper_limit=1.999))\n",
    "\n",
    "# # Calculate errors\n",
    "# k_error = gt_k - pred_k \n",
    "# alpha_error = gt_a - pred_a \n",
    "\n",
    "# # Collect errors by state\n",
    "# for class_idx in range(NUM_CLASSES):\n",
    "#     valid_positions = (gt_state == class_idx) & (gt_state != LABEL_PADDING_VALUE)\n",
    "#     errors_k[class_idx].extend(k_error[valid_positions])\n",
    "#     errors_alpha[class_idx].extend(alpha_error[valid_positions])\n",
    "\n",
    "# # Calculate statistics\n",
    "# mean_k = [np.mean(err) if len(err) > 0 else np.nan for err in errors_k]\n",
    "# mean_alpha = [np.mean(err) if len(err) > 0 else np.nan for err in errors_alpha]\n",
    "# std_k = [np.std(err) if len(err) > 0 else np.nan for err in errors_k]\n",
    "# std_alpha = [np.std(err) if len(err) > 0 else np.nan for err in errors_alpha]\n",
    "\n",
    "# # Calculate 95% confidence intervals\n",
    "# ci_k = np.array([stats.sem(err) * stats.t.ppf((1 + 0.95) / 2, len(err)-1) \n",
    "#                  if len(err) > 1 else np.nan for err in errors_k])\n",
    "# ci_alpha = np.array([stats.sem(err) * stats.t.ppf((1 + 0.95) / 2, len(err)-1) \n",
    "#                     if len(err) > 1 else np.nan for err in errors_alpha])\n",
    "\n",
    "# # Absolute error statistics\n",
    "# abs_mean_k = [np.mean(np.abs(err)) if len(err) > 0 else np.nan for err in errors_k]\n",
    "# abs_mean_alpha = [np.mean(np.abs(err)) if len(err) > 0 else np.nan for err in errors_alpha]\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"\\nError Statistics by State:\")\n",
    "# print(\"-\" * 50)\n",
    "# for state in range(NUM_CLASSES):\n",
    "#     print(f\"\\nState {state}:\")\n",
    "#     print(f\"K error      - Mean ± Std: {mean_k[state]:.3f} ± {std_k[state]:.3f}\")\n",
    "#     print(f\"K error      - Mean ± 95% CI: {mean_k[state]:.3f} ± {ci_k[state]:.3f}\")\n",
    "#     print(f\"K error      - Abs Mean: {abs_mean_k[state]:.3f}\")\n",
    "#     print(f\"Alpha error  - Mean ± Std: {mean_alpha[state]:.3f} ± {std_alpha[state]:.3f}\")\n",
    "#     print(f\"Alpha error  - Mean ± 95% CI: {mean_alpha[state]:.3f} ± {ci_alpha[state]:.3f}\")\n",
    "#     print(f\"Alpha error  - Abs Mean: {abs_mean_alpha[state]:.3f}\")\n",
    "\n",
    "# mean_k = np.array(mean_k)\n",
    "# mean_alpha = np.array(mean_alpha)\n",
    "# std_k = np.array(std_k)\n",
    "# std_alpha = np.array(std_alpha)\n",
    "\n",
    "# # Create figure using Figure and Canvas\n",
    "# fig = Figure(figsize=(10, 6), dpi=300)\n",
    "# canvas = FigureCanvasSVG(fig)\n",
    "# ax = fig.add_subplot(111)\n",
    "# states = np.arange(NUM_CLASSES)\n",
    "\n",
    "# # Create plots with filled circles and dashed error bars\n",
    "# k_line = ax.errorbar(states, mean_k, yerr=std_k, \n",
    "#                     label='K',\n",
    "#                     fmt='o',\n",
    "#                     markerfacecolor=K_COLOR,\n",
    "#                     markeredgecolor=K_COLOR,\n",
    "#                     markeredgewidth=2,\n",
    "#                     capsize=CAPSIZE,\n",
    "#                     # alpha=0.9,\n",
    "#                     color=K_COLOR,\n",
    "#                     markersize=MARKER_SIZE,\n",
    "#                     capthick=LINE_WIDTH,\n",
    "#                     elinewidth=LINE_WIDTH,\n",
    "#                     ls='none')\n",
    "\n",
    "# k_line[-1][0].set_linestyle('--')\n",
    "\n",
    "# alpha_line = ax.errorbar(states, mean_alpha, yerr=std_alpha,\n",
    "#                         label='α',\n",
    "#                         fmt='o',\n",
    "#                         markerfacecolor=ALPHA_COLOR,\n",
    "#                         markeredgecolor=ALPHA_COLOR,\n",
    "#                         markeredgewidth=2,\n",
    "#                         capsize=CAPSIZE,\n",
    "#                         # alpha=0.7,\n",
    "#                         color=ALPHA_COLOR,\n",
    "#                         markersize=MARKER_SIZE,\n",
    "#                         capthick=LINE_WIDTH,\n",
    "#                         elinewidth=LINE_WIDTH,\n",
    "#                         ls='none')\n",
    "\n",
    "# alpha_line[-1][0].set_linestyle('--')\n",
    "\n",
    "# # Calculate y-axis limits with padding\n",
    "# y_min = min(min(mean_k - std_k), min(mean_alpha - std_alpha))\n",
    "# y_max = max(max(mean_k + std_k), max(mean_alpha + std_alpha))\n",
    "# y_range = y_max - y_min\n",
    "# padding = 0.3 * y_range  # 10% padding\n",
    "# ax.set_ylim(y_min - padding, y_max + padding)\n",
    "\n",
    "# # Baseline\n",
    "# ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# # Configure spines\n",
    "# for spine in ax.spines.values():\n",
    "#     spine.set_visible(True)\n",
    "#     spine.set_linewidth(1)\n",
    "#     spine.set_color('black')\n",
    "\n",
    "# ax.set_xlabel('State (s)', fontsize=FONT_SIZE, labelpad=20)  # Updated x-label\n",
    "# ax.set_ylabel(r'$\\overline{Error}$ ± σ', fontsize=FONT_SIZE, labelpad=60)  # Updated y-label with sigma\n",
    "\n",
    "# # Configure ticks\n",
    "# ax.tick_params(which='both', direction='out', length=6, width=1,\n",
    "#               colors='black', pad=2, labelsize=FONT_SIZE)\n",
    "# ax.set_xticks(states)\n",
    "# ax.set_xticklabels([str(i) for i in states])\n",
    "\n",
    "# # Legend\n",
    "# legend = ax.legend(fontsize=FONT_SIZE,\n",
    "#                   frameon=True,\n",
    "#                   edgecolor='black',\n",
    "#                   loc='upper right')\n",
    "# legend.get_frame().set_alpha(1)\n",
    "\n",
    "# # Set transparent background\n",
    "# fig.patch.set_facecolor('none')\n",
    "# ax.set_facecolor('none')\n",
    "\n",
    "# # Layout adjustments\n",
    "# fig.tight_layout(pad=1.0)\n",
    "\n",
    "# # Save using canvas.print_figure\n",
    "# canvas.print_figure('error_by_state.svg',\n",
    "#                    bbox_inches='tight',\n",
    "#                    pad_inches=0.1,\n",
    "#                    format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Length Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "# fig, ax = create_loss_length_plot(gt_a, gt_k, gt_state, pred_a, pred_k, pred_state, ROOT_DIR)\n",
    "fig, ax = create_loss_length_plot_smooth(gt_a, gt_k, gt_state, pred_a, pred_k, pred_state, \"./\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Heatmaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(pred, gt, padding_value=LABEL_PADDING_VALUE):\n",
    "\n",
    "   non_zero_mask = (gt != padding_value)\n",
    "   loss = np.abs(pred - gt) * non_zero_mask\n",
    "\n",
    "   sequence_lengths = non_zero_mask.sum(axis=1)\n",
    "   sequence_sums = loss.sum(axis=1)\n",
    "   sequence_averages = sequence_sums / sequence_lengths\n",
    "\n",
    "   return sequence_averages.mean()\n",
    "\n",
    "male_k = mae(pred_k.copy(), gt_k.copy())\n",
    "mae_alpha = mae(pred_a.copy(), gt_a.copy())\n",
    "\n",
    "print(mae_alpha, male_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example:\n",
    "# # For alpha:\n",
    "# fig, ax = create_heatmap(\n",
    "#     gt_a.flatten(), \n",
    "#     pred_a.flatten(), \n",
    "#     mae_alpha,\n",
    "#     param_name='alpha',\n",
    "#     value_range=(0, 2),\n",
    "#     output_dir=ROOT_DIR,\n",
    "#     color=\"alpha_color\"\n",
    "# )\n",
    "\n",
    "# For K:\n",
    "fig, ax = create_heatmap(\n",
    "    gt_k.flatten(), \n",
    "    pred_k.flatten(), \n",
    "    male_k,\n",
    "    param_name='K',\n",
    "    value_range=(0, 3),\n",
    "    output_dir=ROOT_DIR,\n",
    "    color=\"k_color\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_normalized = calculate_normalized_confusion_matrix(pred_state, gt_state)\n",
    "ax = plot_confusion_matrix(cm_normalized, ROOT_DIR)\n",
    "metrics = calculate_metrics(pred_state, gt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Plots Alpha/K Publication Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'path to jaccard.json'\n",
    "counter_path = 'path to counter.json'\n",
    "# fig, ax = create_jaccard_plot_new(file_path, counter_path, parameter_type='alpha')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Tracks by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "AlphaModel = RegressionModel().to(DEVICE)\n",
    "KModel = RegressionModel().to(DEVICE)\n",
    "StateModel = ClassificationModel().to(DEVICE)\n",
    "\n",
    "AlphaModel.load_state_dict(torch.load(\"models/optimal_weights/alpha_weights_with_fixed\"))\n",
    "KModel.load_state_dict(torch.load(\"models/optimal_weights/k_weights\"))\n",
    "StateModel.load_state_dict(torch.load(\"models/optimal_weights/state_weights\"))\n",
    "\n",
    "AlphaModel.eval()\n",
    "KModel.eval()\n",
    "StateModel.eval()\n",
    "\n",
    "def getPredictions(df, max_size=200):\n",
    "\n",
    "    features = np.nan_to_num(getFeatures(df[\"x\"].values, df[\"y\"].values), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    features = torch.tensor(features, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    length = features.size(1)\n",
    "\n",
    "    if length < max_size:\n",
    "        features = F.pad(features, (0, 0, 0, max_size - length), value=FEATURE_PADDING_VALUE)\n",
    "    elif length > max_size:\n",
    "        features = features[:, :max_size]\n",
    "        print(f\"Note that the input series is longer than the maximum size. The input series has been truncated to the first {max_size} values.\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # convert to numpy arrays for downstream analysis\n",
    "        pred_alpha_list = AlphaModel(features).cpu().numpy().flatten().squeeze()[:length]\n",
    "        pred_k_list = KModel(features).cpu().numpy().flatten().squeeze()[:length]\n",
    "        states_log_probs = StateModel(features)\n",
    "        pred_states_list = torch.argmax(states_log_probs, dim=-1).cpu().numpy().flatten().squeeze()[:length]\n",
    "\n",
    "    pred_alpha_list = np.array(pred_alpha_list)\n",
    "    pred_k_list = np.array(pred_k_list)\n",
    "    pred_states_list = np.array(pred_states_list)\n",
    "\n",
    "    pred_alpha_list = np.clip(median_filter_1d(smooth_series(pred_alpha_list)), 0, 2)\n",
    "    pred_k_list = np.clip(median_filter_1d(smooth_series(pred_k_list)), 0, 6)\n",
    "\n",
    "    return pred_alpha_list, pred_k_list, pred_states_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_state(index=0):\n",
    "    dic = _get_dic_andi2(1)\n",
    "    dic['T'] = 200 \n",
    "    dic['N'] = 100\n",
    "    # dic['alphas'], dic['Ds'] = np.array([0.01, 0]), np.array([500, 0])\n",
    "\n",
    "    dfs_traj, _, _ = challenge_phenom_dataset(save_data=False, \n",
    "                                        dics=[dic], \n",
    "                                        return_timestep_labs=True, \n",
    "                                        get_video=False)\n",
    "\n",
    "    df = dfs_traj[0]\n",
    "    for i in df[\"traj_idx\"].unique():\n",
    "        if len(df[df[\"traj_idx\"] == i]) <= 25:\n",
    "            index = i\n",
    "            break           \n",
    "    return df[df[\"traj_idx\"] == index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = single_state() \n",
    "plt.plot(example_track[\"x\"], example_track[\"y\"])\n",
    "plt.figure()\n",
    "example_track.to_csv(f\"short_trajectory_single_example.csv\", index=False)\n",
    "pa, pk, ps = getPredictions(example_track)\n",
    "create_plots_for_track_examples(example_track, pa, pk, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_state():\n",
    "    dic = _get_dic_andi2(2)\n",
    "    dic['T'] = 200 \n",
    "    dic['N'] = 100\n",
    "    dic['M'] = np.array([[0.5, 0.5],[0.5, 0.5]])\n",
    "    dic['alphas'] = np.array([[1,0],[0.5,0]])\n",
    "    dic[\"Ds\"] = np.array([[1,0],[0.1,0]])\n",
    "    \n",
    "    dfs_traj, _, _ = challenge_phenom_dataset(save_data=False, \n",
    "                                        dics=[dic], \n",
    "                                        return_timestep_labs=True, \n",
    "                                        get_video=False)\n",
    "    \n",
    "    df = dfs_traj[0]\n",
    "\n",
    "    for i in df[\"traj_idx\"].unique():\n",
    "        if len(get_cps_gt(df[df[\"traj_idx\"] == i][\"D\"].values)[1:-1]) == 2 and len(df[df[\"traj_idx\"] == i][\"D\"].values) <=40:\n",
    "            index = i\n",
    "            break\n",
    "\n",
    "    return df[df[\"traj_idx\"] == index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = multi_state() \n",
    "pa, pk, ps = getPredictions(example_track)\n",
    "example_track.to_csv(f\"short_bad_trajectory_multi_example.csv\", index=False)\n",
    "create_plots_for_track_examples(example_track, pa, pk, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confined():\n",
    "    dic = _get_dic_andi2(5)\n",
    "    dic['T'] = 200 \n",
    "    dic['N'] = 100\n",
    "\n",
    "    Ds_array = np.array([[1.5, 0.1],[0.3,0.1]])\n",
    "    alphas_array = np.array([[0.9, 0.1],[0.5,0.1]])\n",
    "    dic['Ds'] = Ds_array\n",
    "    dic['alphas'] = alphas_array\n",
    "    dic['trans'] = 0\n",
    "    dic['Nc'] = random.randint(30,35)\n",
    "    dic['r'] = random.randint(5, 10)\n",
    "    \n",
    "    dfs_traj, _, _ = challenge_phenom_dataset(save_data=False, \n",
    "                                        dics=[dic], \n",
    "                                        return_timestep_labs=True, \n",
    "                                        get_video=False)\n",
    "    df = dfs_traj[0]\n",
    "    \n",
    "    for _, group in df.groupby(\"traj_idx\"):\n",
    "        if len(get_cps_gt(group[\"D\"].values)[1:-1]) == 1:\n",
    "            index = group[\"traj_idx\"].iloc[0]  # Get the actual traj_idx value\n",
    "            break\n",
    "    return df[df[\"traj_idx\"] == index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = confined() \n",
    "pa, pk, ps = getPredictions(example_track)\n",
    "example_track.to_csv(f\"bad_trajectory_confined_example.csv\", index=False)\n",
    "create_plots_for_track_examples(example_track, pa, pk, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def immobile():\n",
    "    dic = _get_dic_andi2(3)\n",
    "    dic['T'] = 200 \n",
    "    dic['N'] = 100\n",
    "\n",
    "    dic['Ds'], dic['alphas'] = np.array([1, 0]), np.array([1, 0])\n",
    "    dic['Pu'] = random.uniform(0, 0.1)\n",
    "    dic['Pb'] = 1\n",
    "    dic['r'] = random.uniform(0.5, 2)\n",
    "    dic['Nt'] = random.randint(100, 300)\n",
    "    \n",
    "    dfs_traj, _, _ = challenge_phenom_dataset(save_data=False, \n",
    "                                        dics=[dic], \n",
    "                                        return_timestep_labs=True, \n",
    "                                        get_video=False)\n",
    "    df = dfs_traj[0]\n",
    "\n",
    "    for _, group in df.groupby(\"traj_idx\"):\n",
    "        if len(get_cps_gt(group[\"D\"].values)[1:-1]) == 5:\n",
    "            index = group[\"traj_idx\"].iloc[0]  # Get the actual traj_idx value\n",
    "            break\n",
    "\n",
    "    return df[df[\"traj_idx\"] == index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = immobile() \n",
    "example_track.to_csv(f\"bad_trajectory_confined_immobile.csv\", index=False)\n",
    "pa, pk, ps = getPredictions(example_track)\n",
    "create_plots_for_track_examples(example_track, pa, pk, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimerised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimerised(index=0):\n",
    "    dic = _get_dic_andi2(4)\n",
    "    dic['T'] = 200 \n",
    "    dic['N'] = 100\n",
    "    Ds_array, alphas_array = np.zeros((2, 2)), np.zeros((2, 2))\n",
    "    Ds_array[0], alphas_array[0] = np.array([1, 0]),  np.array([1, 0])\n",
    "    Ds_array[1], alphas_array[1]  = np.array([1, 0]), np.array([1, 0])\n",
    "    dic['Ds'] = Ds_array\n",
    "    dic['alphas'] = alphas_array\n",
    "    dic['Pu'] = random.uniform(0, 0.1)\n",
    "    dic['Pb'] = 1\n",
    "    dic['r'] = random.uniform(0.5, 5)\n",
    "    \n",
    "    dfs_traj, _, _ = challenge_phenom_dataset(save_data=False, \n",
    "                                        dics=[dic], \n",
    "                                        return_timestep_labs=True, \n",
    "                                        get_video=False)\n",
    "    df = dfs_traj[0]\n",
    "    return df[df[\"traj_idx\"] == index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = dimerised() \n",
    "example_track.to_csv(f\"trajectory_confined_dimerised.csv\", index=False)\n",
    "pa, pk, ps = getPredictions(example_track)\n",
    "create_plots_for_track_examples(example_track, pa, pk, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots from Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# plot_tensorboard_metrics('train_loss.csv', 'val_loss.csv', 'training_plot.svg')\n",
    "# plot_tensorboard_metrics(\"Losses_Training_K_Loss.csv\", \"Losses_Validation_K_Loss.csv\", save_path=\"k_train_plots.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example track ruptures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "# df = multi_state()\n",
    "# data = np.load(\"plots/true_k_ruptures_example.npy\")\n",
    "# print(len(data))\n",
    "# fig, ax = plot_k_prediction_with_ruptures(df, save_path='k_prediction_ruptures_new.svg')\n",
    "\n",
    "# Run the function\n",
    "# df = multi_state()\n",
    "# fig, ax = plot_k_prediction_with_ruptures(df, save_path='k_prediction_ruptures.svg')\n",
    "# print(f\"{fig}, {ax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For alpha variable\n",
    "# plot_time_series_prediction(time_points, alpha_pred, alpha_truth, 'alpha', \n",
    "#                           save_path='alpha_ruptures.svg')\n",
    "\n",
    "# # For K variable\n",
    "# plot_time_series_prediction(time_points, k_pred, k_truth, 'K',\n",
    "#                           save_path='k_ruptures.svg')\n",
    "\n",
    "# # For state variable\n",
    "# plot_time_series_prediction(time_points, state_pred, state_truth, 'state',\n",
    "#                           save_path='state_ruptures.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rasched_andi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
