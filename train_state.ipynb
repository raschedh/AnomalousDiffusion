{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "from utils.pad_batch import pad_batch, LABEL_PADDING_VALUE\n",
    "from models.ClassificationModel import ClassificationModel, NUM_CLASSES\n",
    "import pickle \n",
    "from pathlib import Path\n",
    "from utils.load_data import load_data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 35\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 2e-6\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('The model is running on:', DEVICE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = []\n",
    "val_instances = []\n",
    "test_instances = []\n",
    "\n",
    "train_files = list(Path(\"../data/simulated_tracks\").glob(\"*/train_instances.pkl\"))\n",
    "val_files = list(Path(\"../data/simulated_tracks\").glob(\"*/val_instances.pkl\"))\n",
    "test_files = list(Path(\"../data/simulated_tracks\").glob(\"*/test_instances.pkl\"))\n",
    "\n",
    "for file in train_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        train_instances += pickle.load(f)\n",
    "\n",
    "for file in val_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        val_instances += pickle.load(f)\n",
    "\n",
    "for file in test_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        test_instances += pickle.load(f)\n",
    "\n",
    "print(\"Train data: \", len(train_instances), \"Test data: \", len(test_instances), \"Val data: \", len(val_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_train = ConcatDataset(train_instances)\n",
    "conc_val = ConcatDataset(val_instances)\n",
    "conc_test = ConcatDataset(test_instances)\n",
    "\n",
    "train_loader = DataLoader(conc_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(conc_test, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader = DataLoader(conc_val, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "\n",
    "print(\"DataLoader Sizes:\", len(train_loader), len(test_loader), len(val_loader))\n",
    "\n",
    "# DATA_PATH = \"../data/simulated_tracks\"\n",
    "# filepaths = list(Path(DATA_PATH).rglob('*.parquet'))\n",
    "# random.shuffle(filepaths)\n",
    "\n",
    "# print(\"Number of files found:\", len(filepaths))\n",
    "\n",
    "# train_data = []\n",
    "# test_data = []\n",
    "# val_data = []\n",
    "\n",
    "# train_data = [TimeSeriesDataset(filepath, augment=True) for filepath in filepaths[:int(len(filepaths)*0.7)]]\n",
    "# test_data = [TimeSeriesDataset(filepath, augment=False) for filepath in filepaths[int(len(filepaths)*0.7):int(len(filepaths)*0.85)]]\n",
    "# val_data = [TimeSeriesDataset(filepath, augment=False) for filepath in filepaths[int(len(filepaths)*0.85):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = load_data() \n",
    "\n",
    "print(\"Train data: \", len(train_data))\n",
    "print(\"Val data: \", len(val_data))\n",
    "print(\"Test dataL\", len(test_data))\n",
    "\n",
    "conc_train = ConcatDataset(train_data)\n",
    "conc_val = ConcatDataset(val_data)\n",
    "conc_test = ConcatDataset(test_data)\n",
    "\n",
    "training_loader = DataLoader(conc_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader = DataLoader(conc_val, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(conc_test, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "\n",
    "print(\"Data\", len(training_loader), len(val_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Weights \n",
    "State labels are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = torch.zeros(NUM_CLASSES, dtype=torch.long, device=DEVICE)\n",
    "# progress_bar = tqdm(total=len(training_loader), desc='Weights Finder', position=0)\n",
    "\n",
    "# for _, _, _, state_labels in training_loader:\n",
    "    \n",
    "#     states = state_labels.to(DEVICE).flatten()\n",
    "#     states = states[states != LABEL_PADDING_VALUE]\n",
    "#     class_counts += torch.bincount(states.long(), minlength=class_counts.numel())\n",
    "#     progress_bar.update()\n",
    "\n",
    "# total_samples = class_counts.sum().item()\n",
    "\n",
    "# weights = class_counts.float().reciprocal() * total_samples\n",
    "# normalized_weights = torch.tensor(weights / weights.sum(), device=DEVICE)\n",
    "\n",
    "# progress_bar.close()\n",
    "# print(\"Class Weights (0,1,2,3):\",normalized_weights)\n",
    "\n",
    "# Class Weights (0,1,2,3): tensor([0.1835, 0.1536, 0.0173, 0.6456], device='cuda:1')\n",
    "normalized_weights = torch.tensor([0.1835, 0.1536, 0.0173, 0.6456], device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Load the model, optimizer, scheduler, loss <br>\n",
    "Focal Loss is commented but can be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If using Focal Loss remove the log_softmax from the model as cross_entropy already applies it\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=3):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, output, targets):\n",
    "        \n",
    "#         ce_loss = F.cross_entropy(output, targets, reduction='none', ignore_index=LABEL_PADDING_VALUE)\n",
    "#         mask = (targets != LABEL_PADDING_VALUE).float()\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         focal_weight = (1 - pt) ** self.gamma\n",
    "#         loss = focal_weight * ce_loss * mask\n",
    "\n",
    "#         return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel().to(DEVICE)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('models/checkpoints/state_runs/runs_{}'.format(timestamp))\n",
    "model_directory = os.path.join('models/checkpoints/state_model', 'model_{}'.format(timestamp))\n",
    "print(summary(model, input_size=(BATCH_SIZE, 200, 10)))\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=3)\n",
    "\n",
    "# state_loss_fn = FocalLoss()\n",
    "state_loss_fn = nn.NLLLoss(reduction='mean', weight=normalized_weights.to(DEVICE), ignore_index=LABEL_PADDING_VALUE)\n",
    "state_loss_fn_inference = nn.NLLLoss(reduction='mean', ignore_index=LABEL_PADDING_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    runs = 0\n",
    "\n",
    "    for inputs, _, _ , state_labels in dataloader:\n",
    "\n",
    "        inputs, state_labels = inputs.to(DEVICE), state_labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        state_log_probs  = outputs.permute(0, 2, 1) # output is shape 32,4,500 \n",
    "        loss_state = state_loss_fn(state_log_probs, state_labels.long()) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_state.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        running_loss += loss_state.item()\n",
    "        runs += 1\n",
    "\n",
    "        progress_bar.update()\n",
    "\n",
    "    return running_loss/runs\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    running_val_total = 0.0\n",
    "    val_runs = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, _ , state_labels in dataloader:\n",
    "            \n",
    "            inputs, state_labels = inputs.to(DEVICE), state_labels.to(DEVICE)            \n",
    "            outputs = model(inputs)  \n",
    "            state_log_probs = outputs.permute(0, 2, 1)  # state logs is 32,NUM_CLASSES,200\n",
    "\n",
    "            loss_state = state_loss_fn_inference(state_log_probs, state_labels.long())\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            mask = (state_labels != LABEL_PADDING_VALUE)\n",
    "\n",
    "            all_preds.append(torch.masked_select(predictions, mask))\n",
    "            all_labels.append(torch.masked_select(state_labels, mask))\n",
    "\n",
    "            running_val_total += loss_state.item()\n",
    "            val_runs += 1\n",
    "            progress_bar.update()\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    conf_matrix = 100*np.round(confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3], normalize='true'),2)    \n",
    "\n",
    "    return running_val_total / val_runs, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    progress_bar = tqdm(total=len(training_loader), desc='Training', position=0)\n",
    "\n",
    "    avg_training_loss = train_one_epoch(model, optimizer, training_loader)\n",
    "    val_total_loss, matrix  = evaluate_model(model, val_loader)\n",
    "\n",
    "    print(f'Training LOSS: State {avg_training_loss}\\n'\n",
    "          f'Validation LOSS: State {val_total_loss} \\nState Conf \\n{matrix} \\n')\n",
    "          \n",
    "    writer.add_scalars('Losses', {\n",
    "        'Training Total': avg_training_loss,\n",
    "        'Validation Total': val_total_loss, \n",
    "        }, epoch + 1)\n",
    "\n",
    "    writer.flush()\n",
    "    \n",
    "    if val_total_loss < best_val_loss:\n",
    "        best_val_loss = val_total_loss\n",
    "        \n",
    "    model_path = os.path.join(model_directory, f'model_{epoch + 1}_{int(round(val_total_loss, 2)*100)}')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    scheduler.step(val_total_loss)\n",
    "    \n",
    "progress_bar.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Validation Loss:\", best_val_loss)\n",
    "print(\"Best Model Path\", model_path)\n",
    "\n",
    "# model 19 is best with the weights in the validation loss\n",
    "\n",
    "# without the weights in the validation loss model 15:\n",
    "# Training: 100%|██████████| 105000/105000 [52:46<00:00, 33.16it/s]\n",
    "# Training: 100%|██████████| 105000/105000 [40:21<00:00, 41.51it/s]\n",
    "# Training LOSS: State 0.23815074014839316\n",
    "# Validation LOSS: State 0.3648213621613623 \n",
    "# State Conf \n",
    "# [[99.  0.  0.  0.]\n",
    "#  [ 1. 92.  5.  1.]\n",
    "#  [ 1.  8. 82.  9.]\n",
    "#  [ 0.  1.  6. 93.]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/haidiri/Desktop/AnDiChallenge2024/models/checkpoints/state_model/model_20241001_115850_weighed_on_val/model_25_23\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "running_test_total = 0.0\n",
    "test_runs = 0.0\n",
    "\n",
    "predictions_list = []\n",
    "ground_truth_list = []\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "progress_bar = tqdm(total=len(test_loader), desc='Testing', position=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _, _, state_labels in test_loader:\n",
    "        \n",
    "        inputs, state_labels = inputs.to(DEVICE), state_labels.to(DEVICE)\n",
    "\n",
    "        mask = (state_labels != LABEL_PADDING_VALUE)\n",
    "        outputs = model(inputs).squeeze(-1)\n",
    "\n",
    "        state_log_probs = outputs.permute(0, 2, 1)  # state logs is 32,3,200\n",
    "\n",
    "        loss_state = state_loss_fn_inference(state_log_probs, state_labels.long())\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        all_preds.append(torch.masked_select(predictions, mask))\n",
    "        all_labels.append(torch.masked_select(state_labels, mask))\n",
    "        predictions_list.extend(outputs.cpu().numpy())\n",
    "        ground_truth_list.extend(predictions.cpu().numpy())\n",
    "\n",
    "        running_test_total += loss_state.item()\n",
    "        test_runs += 1\n",
    "        \n",
    "        progress_bar.update()\n",
    "\n",
    "all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "conf_matrix = 100*np.round(confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3], normalize='true'),2)    \n",
    "# Calculate average losses\n",
    "avg_test_loss = running_test_total / test_runs\n",
    "print(f'Average test loss: {avg_test_loss}')\n",
    "print(conf_matrix)\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((100+91+82+94)/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 53\n",
    "\n",
    "padding_starts = (ground_truth[INDEX] == LABEL_PADDING_VALUE).argmax() \n",
    "\n",
    "if padding_starts == 0:\n",
    "    padding_starts = 200\n",
    "\n",
    "pred_alpha = predictions[INDEX][:padding_starts]\n",
    "true_alpha = ground_truth[INDEX][:padding_starts]\n",
    "\n",
    "plt.scatter([i for i in range(len(pred_alpha))], pred_alpha, color=\"red\")\n",
    "plt.scatter([i for i in range(len(true_alpha))], true_alpha, color=\"blue\")\n",
    "plt.title(\"Alpha\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rasched_andi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
