{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import os \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "from utils.pad_batch import pad_batch, LABEL_PADDING_VALUE\n",
    "from models.RegressionModel import RegressionModel\n",
    "import pickle \n",
    "from pathlib import Path\n",
    "from utils.load_data import load_data\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 35\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 2e-6\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('The model is running on:', DEVICE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = []\n",
    "val_instances = []\n",
    "test_instances = []\n",
    "\n",
    "train_files = list(Path(\"../data/simulated_tracks\").glob(\"*/train_instances.pkl\"))\n",
    "val_files = list(Path(\"../data/simulated_tracks\").glob(\"*/val_instances.pkl\"))\n",
    "test_files = list(Path(\"../data/simulated_tracks\").glob(\"*/test_instances.pkl\"))\n",
    "\n",
    "for file in train_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        train_instances += pickle.load(f)\n",
    "\n",
    "for file in val_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        val_instances += pickle.load(f)\n",
    "\n",
    "for file in test_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        test_instances += pickle.load(f)\n",
    "\n",
    "print(\"Train data: \", len(train_instances), \"Test data: \", len(test_instances), \"Val data: \", len(val_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_train = ConcatDataset(train_instances)\n",
    "conc_val = ConcatDataset(val_instances)\n",
    "conc_test = ConcatDataset(test_instances)\n",
    "\n",
    "train_loader = DataLoader(conc_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(conc_test, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader = DataLoader(conc_val, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "\n",
    "print(\"DataLoader Sizes:\", len(train_loader), len(test_loader), len(val_loader))\n",
    "\n",
    "# DATA_PATH = \"../data/simulated_tracks\"\n",
    "# filepaths = list(Path(DATA_PATH).rglob('*.parquet'))\n",
    "# random.shuffle(filepaths)\n",
    "\n",
    "# print(\"Number of files found:\", len(filepaths))\n",
    "\n",
    "# train_data = []\n",
    "# test_data = []\n",
    "# val_data = []\n",
    "\n",
    "# train_data = [TimeSeriesDataset(filepath, augment=True) for filepath in filepaths[:int(len(filepaths)*0.7)]]\n",
    "# test_data = [TimeSeriesDataset(filepath, augment=False) for filepath in filepaths[int(len(filepaths)*0.7):int(len(filepaths)*0.85)]]\n",
    "# val_data = [TimeSeriesDataset(filepath, augment=False) for filepath in filepaths[int(len(filepaths)*0.85):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _, test_data = load_data() \n",
    "\n",
    "print(\"Train data: \", len(train_data))\n",
    "# print(\"Val data: \", len(val_data))\n",
    "print(\"Test data: \", len(test_data))\n",
    "\n",
    "\n",
    "conc_train = ConcatDataset(train_data)\n",
    "# conc_val = ConcatDataset(val_data)\n",
    "conc_test = ConcatDataset(test_data)\n",
    "\n",
    "training_loader = DataLoader(conc_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "# val_loader = DataLoader(conc_val, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "test_loader = DataLoader(conc_test, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "\n",
    "# print(\"Data\", len(training_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Load the model, optimizer, scheduler, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel().to(DEVICE)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('models/checkpoints/k_runs/runs_{}'.format(timestamp))\n",
    "model_directory = os.path.join('models/checkpoints/k_model', 'model_{}'.format(timestamp))\n",
    "    \n",
    "print(summary(model, input_size=(BATCH_SIZE, 200, 10)))\n",
    "\n",
    "continuous_loss_fn = nn.L1Loss(reduction='none')\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    runs = 0\n",
    "\n",
    "    for inputs, _,k_labels,_ in dataloader:\n",
    "\n",
    "        inputs, k_labels = inputs.to(DEVICE), k_labels.to(DEVICE)\n",
    "        mask = (k_labels != LABEL_PADDING_VALUE).float()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        total_loss = (continuous_loss_fn(outputs, k_labels) * mask).sum() / mask.sum()\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        runs += 1\n",
    "\n",
    "        progress_bar.update()\n",
    "\n",
    "    return running_loss/runs\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    running_val_total = 0.0\n",
    "    val_runs = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, k_labels,_ in dataloader:\n",
    "            \n",
    "            inputs, k_labels = inputs.to(DEVICE), k_labels.to(DEVICE)\n",
    "            mask = (k_labels != LABEL_PADDING_VALUE).float()\n",
    "            \n",
    "            outputs = model(inputs)  \n",
    "            outputs = outputs.squeeze(-1)\n",
    "            total_loss = (continuous_loss_fn(outputs, k_labels) * mask).sum() / mask.sum()            \n",
    "            running_val_total += total_loss.item()\n",
    "            val_runs += 1\n",
    "    \n",
    "    return running_val_total / val_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/home/haidiri/Desktop/AnDiChallenge2024/src/models/checkpoints/k_model/model_20240923_161109/model_35\"))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    progress_bar = tqdm(total=len(training_loader), desc='Training', position=0)\n",
    "\n",
    "    avg_training_loss = train_one_epoch(model, optimizer, training_loader)\n",
    "    val_total_loss  = evaluate_model(model, val_loader)\n",
    "    \n",
    "    print(f'Training LOSS: K {avg_training_loss}\\n'\n",
    "          f'Validation LOSS: K {val_total_loss} \\n')\n",
    "    \n",
    "    writer.add_scalars('Losses', {\n",
    "        'Training K Loss': avg_training_loss,\n",
    "        'Validation K Loss': val_total_loss,\n",
    "        }, epoch + 1)\n",
    "\n",
    "    writer.flush()\n",
    "    \n",
    "    if val_total_loss < best_val_loss:\n",
    "        best_val_loss = val_total_loss\n",
    "        best_model_path = os.path.join(model_directory, f'model_{epoch + 1}')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    scheduler.step(val_total_loss)\n",
    "    \n",
    "progress_bar.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Validation Loss:\", best_val_loss)\n",
    "print(\"Best Model Path\", best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/home/haidiri/Desktop/AnDiChallenge2024/models/optimal_weights/k_weights\"))\n",
    "model.eval()\n",
    "\n",
    "running_test_total = 0.0\n",
    "test_runs = 0.0\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "progress_bar = tqdm(total=len(test_loader), desc='Testing', position=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _, k_labels,_ in test_loader:\n",
    "        \n",
    "        inputs, k_labels = inputs.to(DEVICE), k_labels.to(DEVICE)\n",
    "\n",
    "        mask = (k_labels != LABEL_PADDING_VALUE).float()\n",
    "        outputs = model(inputs).squeeze(-1)\n",
    "        total_loss = (continuous_loss_fn(outputs, k_labels) * mask).sum() / mask.sum()\n",
    "        \n",
    "        running_test_total += total_loss.item()\n",
    "        test_runs += 1\n",
    "\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        ground_truth.extend(k_labels.cpu().numpy())\n",
    "        progress_bar.update()\n",
    "\n",
    "\n",
    "# Calculate average losses\n",
    "avg_test_loss = running_test_total / test_runs\n",
    "print(f'Average test loss: {avg_test_loss}')\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postprocessing import smooth_series, median_filter_1d\n",
    "import ruptures as rpt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCP_rpt(array, lower_limit=0, upper_limit=float(\"inf\"), threshold=0.05):\n",
    "    array = median_filter_1d(smooth_series(array, lower_limit=lower_limit, upper_limit=upper_limit))\n",
    "    if np.max(array) != np.min(array):\n",
    "        pred_series_scaled = (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    else:\n",
    "        pred_series_scaled = np.ones(len(array)) * 0.5 #scale them to default value of 0.5\n",
    "\n",
    "    algo = rpt.Pelt(model=\"l2\", min_size=3, jump=1).fit(pred_series_scaled)\n",
    "    cps = [0] + algo.predict(pen=0.3)\n",
    "\n",
    "    remove = []\n",
    "    for i in range(1, len(cps) - 1):\n",
    "        left_mean = array[cps[i - 1]:cps[i]].mean()\n",
    "        right_mean = array[cps[i]:cps[i + 1]].mean()        \n",
    "        if abs(left_mean - right_mean) < threshold:\n",
    "            remove.append(cps[i])\n",
    "    \n",
    "    cps = [cp for cp in cps if cp not in remove]\n",
    "\n",
    "    return cps, array\n",
    "\n",
    "def getCP_gt(array):\n",
    "    cps = [0]\n",
    "    for i in range(1, len(array)):\n",
    "        if array[i-1] != array[i]:\n",
    "            cps.append(i)\n",
    "\n",
    "    return cps + [len(array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 1086\n",
    "\n",
    "# for idx in range(len(ground_truth)):\n",
    "\n",
    "#     padding_starts = (ground_truth[idx] == LABEL_PADDING_VALUE).argmax() \n",
    "\n",
    "#     if padding_starts == 0:\n",
    "#         padding_starts = 200\n",
    "\n",
    "#     pred_k = predictions[idx][:padding_starts]\n",
    "#     true_k = ground_truth[idx][:padding_starts]\n",
    "\n",
    "#     diff = np.diff(pred_k)\n",
    "#     diff[diff !=0] = 1\n",
    "#     changepoints = int(np.sum(diff))\n",
    "\n",
    "#     cp_pred, _ = getCP_rpt(pred_k, lower_limit=0, upper_limit=6, threshold=0.05)\n",
    "#     cp_gt = getCP_gt(true_k)\n",
    "\n",
    "#     if len(true_k) == 200 and changepoints == 3 and cp_pred == cp_gt and 0 not in true_k:\n",
    "#         print(idx)\n",
    "#         break\n",
    "\n",
    "pred_k = predictions[INDEX]\n",
    "true_k = ground_truth[INDEX]\n",
    "\n",
    "# Save to numpy files\n",
    "# np.save('pred_k_ruptures_example.npy', pred_k)\n",
    "# np.save('true_k_ruptures_example.npy', true_k)\n",
    "\n",
    "cp_pred, pred_k = getCP_rpt(pred_k, lower_limit=0, upper_limit=6, threshold=0.05)\n",
    "cp_gt = getCP_gt(true_k)\n",
    "\n",
    "cp_pred = cp_pred[1:-1]\n",
    "print(cp_pred, cp_gt)\n",
    "\n",
    "for i in cp_pred:\n",
    "    plt.axvline(x=i, color='black', linestyle='--')\n",
    "    \n",
    "# plt.plot(cp_gt, true_k[cp_gt], color='black', linestyle='-')\n",
    "\n",
    "plt.scatter([i for i in range(len(pred_k))], pred_k, color=\"red\")\n",
    "plt.scatter([i for i in range(len(true_k))], true_k, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_svg import FigureCanvasSVG\n",
    "\n",
    "def plot_k_with_ruptures(pred_k, true_k, cp_pred, cp_gt, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication quality plot comparing K predictions and ground truth with rupture points\n",
    "    \"\"\"\n",
    "    # Create figure with specific DPI for precise control\n",
    "    fig = Figure(figsize=(8, 6), dpi=300)\n",
    "    canvas = FigureCanvasSVG(fig)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    time = np.arange(len(pred_k))\n",
    "    \n",
    "    # Plot predictions with green open circles\n",
    "    ax.plot(time, pred_k, color='#1B9E77', linestyle='none', \n",
    "            marker='o', markersize=6, markerfacecolor='none',\n",
    "            markeredgecolor='#1B9E77', markeredgewidth=1.5,\n",
    "            alpha=0.7)\n",
    "    \n",
    "    # Plot ground truth as separate horizontal lines between change points\n",
    "    for i in range(len(cp_gt)-1):\n",
    "        start_idx = cp_gt[i]\n",
    "        end_idx = cp_gt[i+1]\n",
    "        if end_idx == len(true_k):\n",
    "            end_idx -= 1\n",
    "        segment_value = true_k[start_idx]  # value for this segment\n",
    "        ax.plot([start_idx, end_idx], [segment_value, segment_value],\n",
    "                color='black', linewidth=2, alpha=1)\n",
    "    \n",
    "    # Plot rupture lines with increased prominence\n",
    "    for cp in cp_pred:\n",
    "        ax.axvline(x=cp, color='black', linestyle='--', \n",
    "                  linewidth=3, alpha=0.8)\n",
    "    \n",
    "    # Set y-axis limits with a small margin\n",
    "    ymin = min(min(pred_k), min(true_k))\n",
    "    ymax = max(max(pred_k), max(true_k))\n",
    "    margin = (ymax - ymin) * 0.1  # 10% margin\n",
    "    ax.set_ylim(ymin - margin, ymax + margin)\n",
    "    \n",
    "    # Configure axis and style\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Configure tick parameters to exactly match heatmap\n",
    "    ax.tick_params(which='both', direction='out', length=6, width=1,\n",
    "                  colors='black', pad=2)\n",
    "    \n",
    "    # Set tick label sizes to exactly match heatmap\n",
    "    ax.tick_params(axis='both', labelsize=32)\n",
    "    \n",
    "    ax.plot([], [], color='#1B9E77', linestyle='none',\n",
    "            marker='o', markersize=12, markerfacecolor='none',\n",
    "            markeredgecolor='#1B9E77', markeredgewidth=1.5,\n",
    "            label='Prediction')\n",
    "    \n",
    "    ax.plot([], [], color='black', linewidth=2, label='Ground Truth')\n",
    "\n",
    "    # Configure spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Set labels with exact same formatting as heatmap\n",
    "    ax.set_xlabel('Time', fontsize=32)\n",
    "    ax.set_ylabel(r'$K$', fontsize=32)\n",
    "    \n",
    "    # Configure legend with matching font size\n",
    "    ax.legend(fontsize=28, frameon=True, loc='best')\n",
    "    \n",
    "    # Adjust layout with same padding\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    \n",
    "    # Save as SVG with same parameters\n",
    "    if save_path:\n",
    "        canvas.print_figure(save_path, bbox_inches='tight', \n",
    "                          pad_inches=0.1, format='svg')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Example usage\n",
    "plot_k_with_ruptures(pred_k, true_k, cp_pred, cp_gt, save_path='k_ruptures_example.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Plots for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(ground_truth)):\n",
    "    padding_starts = (ground_truth[idx] == LABEL_PADDING_VALUE).argmax() \n",
    "    if padding_starts == 0:\n",
    "        padding_starts = 200\n",
    "\n",
    "    pred_k = predictions[idx][:padding_starts]\n",
    "    true_k = ground_truth[idx][:padding_starts]\n",
    "\n",
    "    diff = np.diff(pred_k)\n",
    "    diff[diff !=0] = 1\n",
    "    changepoints = int(np.sum(diff))\n",
    "\n",
    "    cp_pred, _ = getCP_rpt(pred_k, lower_limit=0, upper_limit=6, threshold=0.05)\n",
    "    cp_gt = getCP_gt(true_k)\n",
    "\n",
    "    if len(true_k) == 200 and changepoints == 1 and 0 in true_k:\n",
    "        print(idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bound = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_with_ruptures(pred, true, label_type, save_path=None):\n",
    "    K_COLOR = '#1B9E77'      # Green\n",
    "    ALPHA_COLOR = '#E69F00'  # Orange\n",
    "    STATE_COLOR = '#9970AB'  # Purple\n",
    "    \n",
    "    if label_type ==\"alpha\":\n",
    "        COLOR = ALPHA_COLOR \n",
    "        ymin = 0\n",
    "        ymax = 2\n",
    "        label = r'$K$'\n",
    "    elif label_type == \"k\":\n",
    "        COLOR = K_COLOR\n",
    "        ymin = 0\n",
    "        ymax = 3\n",
    "        label = r'$α$'\n",
    "    elif label_type == \"state\":\n",
    "        COLOR = STATE_COLOR\n",
    "        ymin = 0\n",
    "        ymax = 4\n",
    "        label = r'$s$'\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    def getCP_gt(array):\n",
    "        cps = [0]\n",
    "        for i in range(1, len(array)):\n",
    "            if array[i-1] != array[i]:\n",
    "                cps.append(i)\n",
    "\n",
    "        return cps + [len(array)]\n",
    "\n",
    "    fig = Figure(figsize=(8, 6), dpi=300)\n",
    "    canvas = FigureCanvasSVG(fig)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    time = np.arange(len(pred))\n",
    "    ax.plot(time, pred, color=COLOR, linestyle='none', \n",
    "            marker='o', markersize=6, markerfacecolor='none',\n",
    "            markeredgecolor='#1B9E77', markeredgewidth=1.5,\n",
    "            alpha=0.7)\n",
    "    \n",
    "    cp_gt = getCP_gt(true)\n",
    "\n",
    "    for i in range(len(cp_gt)-1):\n",
    "        start_idx = cp_gt[i]\n",
    "        end_idx = cp_gt[i+1]\n",
    "        if end_idx == len(true):\n",
    "            end_idx -= 1\n",
    "        segment_value = true[start_idx]  # value for this segment\n",
    "        ax.plot([start_idx, end_idx], [segment_value, segment_value],\n",
    "                color='black', linewidth=2, alpha=1)\n",
    "\n",
    "    margin = (ymax - ymin) * 0.1  # 10% margin\n",
    "    ax.set_ylim(ymin - margin, ymax + margin)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.tick_params(which='both', direction='out', length=6, width=1,\n",
    "                  colors='black', pad=2)\n",
    "    \n",
    "    ax.tick_params(axis='both', labelsize=32)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Set labels with exact same formatting as heatmap\n",
    "    ax.set_xlabel('Time', fontsize=32)\n",
    "    ax.set_ylabel(label, fontsize=32)\n",
    "    \n",
    "    # # Configure legend with matching font size\n",
    "    # ax.legend(fontsize=28, frameon=True, loc='best')\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    \n",
    "    # Save as SVG with same parameters\n",
    "    if save_path:\n",
    "        canvas.print_figure(save_path, bbox_inches='tight', \n",
    "                          pad_inches=0.1, format='svg')\n",
    "    \n",
    "    return fig, ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gapseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
